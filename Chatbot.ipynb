{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import lightning as L\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import logging\n",
    "# Set the logging level to ERROR to suppress INFO and WARNING messages\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "df = pd.read_csv('llm_input.csv', usecols=[\"instruction\", \"response\"])\n",
    "data = list(tuple(zip(df[\"instruction\"], df[\"response\"])))\n",
    "\n",
    "\n",
    "def build_vocab(data):\n",
    "    vocab = {\"<PAD>\": 0, \"<UNK>\": 1, \"<SOS>\": 2, \"<EOS>\": 3}\n",
    "    for question, answer in data:\n",
    "        for word in question.split() + answer.split():\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "    id_to_token = {v: k for k, v in vocab.items()} # Reverse token and id\n",
    "    return vocab, id_to_token\n",
    "\n",
    "\n",
    "vocab, id_to_token = build_vocab(data)\n",
    "\n",
    "\n",
    "def encode_input(question, answer, vocab, max_len):\n",
    "    \"\"\"Encodes both question and answer into a single list of token IDs, \n",
    "       separated by a [EOS] token, padding to max_len.\n",
    "    \"\"\"\n",
    "    \n",
    "    question_encoded = [vocab.get(word, vocab[\"<UNK>\"]) for word in question.split()]\n",
    "    answer_encoded = [vocab.get(word, vocab[\"<UNK>\"]) for word in answer.split()]\n",
    "    \n",
    "    # Combine question and answer with [EOS] token\n",
    "    encoded = [vocab[\"<SOS>\"]] + question_encoded + [vocab[\"<EOS>\"]] + answer_encoded + [vocab[\"<EOS>\"]]\n",
    "    \n",
    "    # Padding\n",
    "    padding_lenth = max_len - len(encoded)\n",
    "    if padding_lenth > 0:\n",
    "        encoded += [vocab[\"<PAD>\"]] * padding_lenth\n",
    "    \n",
    "    # Truncate if longer than max length AND remove the last token \n",
    "    # to prepare for teacher forcing\n",
    "    return encoded[:max_len]\n",
    "\n",
    "\n",
    "def encode_label(question, answer, vocab, max_len):\n",
    "    \"\"\"Encodes both question and answer into a single list of token IDs, \n",
    "       separated by a [EOS] token, padding to max_len.\n",
    "    \"\"\"\n",
    "    \n",
    "    question_encoded = [vocab.get(word, vocab[\"<UNK>\"]) for word in question.split()]\n",
    "    answer_encoded = [vocab.get(word, vocab[\"<UNK>\"]) for word in answer.split()]\n",
    "    \n",
    "    # Combine question and answer with [EOS] token\n",
    "    # Labels remove the first character of the sequences (pos 1) and keep the <SOS> character\n",
    "    encoded = [vocab[\"<SOS>\"]] + question_encoded[1:] + [vocab[\"<EOS>\"]] + answer_encoded + [vocab[\"<EOS>\"]]\n",
    "    \n",
    "    # Padding\n",
    "    padding_lenth = max_len - len(encoded)\n",
    "    if padding_lenth > 0:\n",
    "        encoded += [vocab[\"<PAD>\"]] * padding_lenth\n",
    "    \n",
    "    # Truncate if longer than max length AND remove the last token \n",
    "    # to prepare for teacher forcing\n",
    "    return encoded[:max_len]\n",
    "\n",
    "\n",
    "inputs = torch.tensor([encode_input(question, answer, vocab, 100) for question, answer in data])\n",
    "labels = torch.tensor([encode_label(question, answer, vocab, 100) for question, answer in data]) \n",
    "\n",
    "dataset = TensorDataset(inputs, labels)\n",
    "dataloader = DataLoader(dataset)\n",
    "\n",
    "\n",
    "\n",
    "class PositionEncoding(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model=10, max_len=100):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        \n",
    "        position = torch.arange(start=0, end=max_len, step=1).float().unsqueeze(1)\n",
    "        \n",
    "        embedding_index = torch.arange(start=0, end=d_model, step=2).float()\n",
    "        \n",
    "        div_term = 1/torch.tensor(10000.0)**(embedding_index/d_model)\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "\n",
    "    def forward(self, word_embeddings):\n",
    "        \n",
    "        return word_embeddings + self.pe[:word_embeddings.size(0)]\n",
    "    \n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model=2):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        \n",
    "        self.row_dim = 0\n",
    "        self.col_dim = 1\n",
    "        \n",
    "    def forward(self, encodings_for_q, encodings_for_k, encodings_for_v, mask=None):\n",
    "        \n",
    "        q = self.W_q(encodings_for_q)\n",
    "        k = self.W_k(encodings_for_k)\n",
    "        v = self.W_v(encodings_for_v)\n",
    "        \n",
    "        sims = torch.matmul(q, k.transpose(dim0=self.row_dim, dim1=self.col_dim))\n",
    "\n",
    "        scaled_sims = sims / torch.sqrt(torch.tensor(self.d_model))\n",
    "\n",
    "        if mask is not None:\n",
    "            \n",
    "            scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e9)\n",
    "            \n",
    "        attention_percents = F.softmax(scaled_sims, dim=self.col_dim)\n",
    "        \n",
    "        attention_scores = torch.matmul(attention_percents, v)\n",
    "        \n",
    "        return attention_scores\n",
    "    \n",
    "\n",
    "\n",
    "class DecoderOnlyTransformer(L.LightningModule):\n",
    "    \n",
    "    def __init__(self, num_tokens=len(vocab), d_model=10, max_len=100):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        L.seed_everything(seed=42)\n",
    "        \n",
    "        self.we = nn.Embedding(num_embeddings=num_tokens,\n",
    "                               embedding_dim=d_model)\n",
    "        \n",
    "        self.pe = PositionEncoding(d_model=d_model, \n",
    "                                   max_len=max_len)\n",
    "        \n",
    "        self.self_attention = Attention(d_model=d_model)\n",
    "        \n",
    "        self.fc_layer = nn.Linear(in_features=d_model, out_features=num_tokens)\n",
    "        \n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    \n",
    "    def forward(self, token_ids):\n",
    "        \n",
    "        word_embeddings = self.we(token_ids)\n",
    "        \n",
    "        position_encodings = self.pe(word_embeddings)\n",
    "        \n",
    "        mask = torch.tril(torch.ones((token_ids.size(dim=0), token_ids.size(dim=0)), device=self.device))\n",
    "        \n",
    "        mask = mask == 0\n",
    "        \n",
    "        self_attention_values = self.self_attention(encodings_for_q=position_encodings,\n",
    "                                                    encodings_for_k=position_encodings,\n",
    "                                                    encodings_for_v=position_encodings,\n",
    "                                                    mask=mask)\n",
    "        \n",
    "        residual_connection_values = position_encodings + self_attention_values\n",
    "        \n",
    "        fc_layer_output = self.fc_layer(residual_connection_values)\n",
    "        \n",
    "        return fc_layer_output\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        return Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_tokens, labels = batch\n",
    "        input_tokens, labels = input_tokens.to(self.device), labels.to(self.device)\n",
    "        output = self.forward(input_tokens[0])\n",
    "        loss = self.loss(output, labels[0])\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train And Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3050 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\ROOTDIR\\python\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type             | Params | Mode \n",
      "------------------------------------------------------------\n",
      "0 | we             | Embedding        | 155 K  | train\n",
      "1 | pe             | PositionEncoding | 0      | train\n",
      "2 | self_attention | Attention        | 300    | train\n",
      "3 | fc_layer       | Linear           | 171 K  | train\n",
      "4 | loss           | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------------\n",
      "327 K     Trainable params\n",
      "0         Non-trainable params\n",
      "327 K     Total params\n",
      "1.309     Total estimated model params size (MB)\n",
      "e:\\ROOTDIR\\python\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5db940701d0a45afa5af226b45f5baea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    }
   ],
   "source": [
    "model = DecoderOnlyTransformer(num_tokens=len(vocab), d_model=10, max_len=100)\n",
    "model = model.to(device)\n",
    "\n",
    "trainer = L.Trainer(max_epochs=50)\n",
    "trainer.fit(model, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'llm.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: status of your account on the current status of your refund. To provide accurate information, could you please provide accurate information, I will help you with the necessary information, I will help me with the status of your compensation. Your satisfaction is our priority, and we will help me to assist you. Thank you for you. <EOS> "
     ]
    }
   ],
   "source": [
    "model = DecoderOnlyTransformer()\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "model_path = 'llm.pth'\n",
    "model.load_state_dict(torch.load(model_path, weights_only=False))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "def text_to_tensor(input_text):\n",
    "    tokens = input_text.split()\n",
    "    token_ids = []\n",
    "    for token in tokens:\n",
    "        if token in vocab:\n",
    "            token_ids.append(vocab[token])\n",
    "        else:\n",
    "            print(f\"Warning: Token '{token}' not found in dictionary.\")\n",
    "    return torch.tensor(token_ids)\n",
    "\n",
    "\n",
    "input_text = input(\"Prompt: \")\n",
    "model_input = text_to_tensor(input_text)\n",
    "\n",
    "input_length = model_input.size(dim=0)\n",
    "\n",
    "predictions = model(model_input)\n",
    "predicted_id = torch.tensor([torch.argmax(predictions[-1, :])])\n",
    "predicted_ids = predicted_id\n",
    "\n",
    "max_length = 100\n",
    "\n",
    "for i in range(input_length, max_length):\n",
    "    if (predicted_id == vocab['<EOS>']):\n",
    "        break\n",
    "    model_input = torch.cat((model_input, predicted_id))\n",
    "    predictions = model(model_input)\n",
    "    predicted_id = torch.tensor([torch.argmax(predictions[-1, :])])\n",
    "    predicted_ids = torch.cat((predicted_ids, predicted_id))\n",
    "\n",
    "\n",
    "print(\"Answer:\", end=\" \")    \n",
    "for id in predicted_ids:\n",
    "    print(id_to_token[id.item()], end=\" \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
